---
title: "math tools"
---

## Derivative Rules

Derivatives are essential for understanding rates of change, especially in economics for optimizing functions like cost, revenue, and utility. Here are the core rules:

1.  **Power Rule**: If $f(x) = x^n$, then $f'(x) = n \cdot x^{n-1}$.
2.  **Constant Rule**: If $f(x) = c$ (where $c$ is a constant), then $f'(x) = 0$.
3.  **Product Rule**: If $f(x) = g(x) \cdot h(x)$, then $f'(x) = g(x) \cdot h'(x) + h(x) \cdot g'(x)$.
4.  **Quotient Rule**: If $f(x) = \frac{g(x)}{h(x)}$, then $f'(x) = \frac{g'(x) \cdot h(x) - g(x) \cdot h'(x)}{[h(x)]^2}$.
5.  **Chain Rule**: If $f(x) = g(h(x))$, then $f'(x) = g'(h(x)) \cdot h'(x)$.

### Special Derivative Cases

-   **Exponential Function**: If $f(x) = e^x$, then $f'(x) = e^x$. If $f(x) = e^{2x}$, then $f'(x) = 2e^{2x}$.
-   **Logarithmic Function**: If $f(x) = \ln(x)$, then $f'(x) = \frac{1}{x}$.

These rules provide the foundation for differentiating complex functions and are widely used in economic analysis, from marginal cost calculations to elasticity evaluations.

## Exponent Rules

Exponential rules are useful for simplifying functions in economic analysis, particularly in growth models and elasticity calculations. Here are the fundamental rules:

1.  **Product of Powers**: $a^m \cdot a^n = a^{m+n}$
2.  **Quotient of Powers**: $\frac{a^m}{a^n} = a^{m-n}$ (where $a \neq 0$)
3.  **Power of a Power**: $(a^m)^n = a^{m \cdot n}$
4.  **Power of a Product**: $(a \cdot b)^n = a^n \cdot b^n$
5.  **Power of a Quotient**: $\left(\frac{a}{b}\right)^n = \frac{a^n}{b^n}$ (where $b \neq 0$)

### Special Exponent Cases

-   **Zero Exponent**: $a^0 = 1$ (where $a \neq 0$)
-   **Negative Exponent**: $a^{-n} = \frac{1}{a^n}$ (where $a \neq 0$)
-   **Fractional Exponent**: $a^{\frac{m}{n}} = \sqrt[n]{a^m}$

These rules help simplify complex expressions, especially in compound interest calculations and growth models.

## Logarithmic Laws

Logarithmic functions are essential tools in economics, especially for transforming data and simplifying multiplicative relationships. Here are the key laws:

1.  **Product Rule**: $\log_b(xy) = \log_b(x) + \log_b(y)$
2.  **Quotient Rule**: $\log_b\left(\frac{x}{y}\right) = \log_b(x) - \log_b(y)$
3.  **Power Rule**: $\log_b(x^k) = k \cdot \log_b(x)$
4.  **Change of Base Formula**: $\log_b(x) = \frac{\log_k(x)}{\log_k(b)}$ (useful for switching bases)

### Special Logarithmic Properties

-   **Log of 1**: $\log_b(1) = 0$
-   **Log of the Base**: $\log_b(b) = 1$
-   **Inverse Property**: $b^{\log_b(x)} = x$

## Unconstrained Optimization

### Single Variable

1.  **Critical Point**: For functions like a profit function $π(q)$, find the critical point by setting the first derivative (FOC) to zero. Example: ( $\frac{d\pi(q)}{dq} = 0$).
2.  **Second Order Condition**: Use the second derivative to determine if it's a max or min. If ($\frac{d^2 \pi}{dq^2} < 0$), it’s a local max.

### Multiple Variables

1.  **Partial Derivatives**: For multivariable functions, set partial derivatives with respect to each variable to zero to find critical points.
2.  **Hessian Matrix**: For max/min, check if the Hessian matrix is positive or negative definite.

## Implicit Function Theorem & Envelope Theorem

-   **Implicit Function Theorem**: Determines how a function changes with parameters by simplifying derivatives.
-   **Envelope Theorem**: For optimized functions, it simplifies the derivative calculation to focus on the direct effect.

## Constrained Optimization

1.  **Lagrange Method**: For maximizing under constraints, use Lagrangian ( $L = f(x) + \lambda g(x)$ ) with FOCs.
2.  **Example**: Maximize area of a rectangular fence, given perimeter constraint ( $2x + 2y = p$ ).

## Linear Algebra

Linear algebra is foundational for economic modeling, especially in optimization, econometrics, and data analysis. Here are some essential concepts:

### Vectors and Matrices

-   **Vector**: An ordered list of numbers, typically denoted as $v = [v_1, v_2, \dots, v_n]$.
-   **Matrix**: A rectangular array of numbers with dimensions $m \times n$, typically written as $A = [a_{ij}]$, where $a_{ij}$ represents the element in the $i^{th}$ row and $j^{th}$ column.

### Matrix Operations

1.  **Addition and Subtraction**: Matrices of the same dimensions can be added or subtracted element-wise. If $A$ and $B$ are $m \times n$ matrices, then $(A + B)_{ij} = a_{ij} + b_{ij}$.

    Example: $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, $B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}$, then $A + B = \begin{bmatrix} 6 & 8 \\ 10 & 12 \end{bmatrix}$.

2.  **Scalar Multiplication**: Multiplying a matrix by a scalar $c$ scales each element by $c$, i.e., $(cA)_{ij} = c \cdot a_{ij}$.

    Example: $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, then $2A = \begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix}$.

3.  **Matrix Multiplication**: For matrices $A$ of dimensions $m \times n$ and $B$ of dimensions $n \times p$, the product $AB$ is an $m \times p$ matrix where $(AB)_{ij} = \sum_{k=1}^{n} a_{ik} \cdot b_{kj}$.

    Example: $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, $B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}$, then $AB = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix}$.

### Special Matrices

-   **Identity Matrix**: A square matrix with ones on the diagonal and zeros elsewhere, denoted $I$. For any matrix $A$, $AI = IA = A$.

    Example: $I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.

-   **Transpose**: The transpose of a matrix $A$, denoted $A^T$, swaps its rows and columns.

    Example: If $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, then $A^T = \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix}$.

-   **Inverse**: For a square matrix $A$, the inverse $A^{-1}$ (if it exists) satisfies $A \cdot A^{-1} = I$.

    Example: If $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, then $A^{-1} = \begin{bmatrix} -2 & 1 \\ 1.5 & -0.5 \end{bmatrix}$.

### Determinants and Rank

-   **Determinant**: A scalar value associated with a square matrix, denoted $\det(A)$ or $|A|$, which indicates whether a matrix is invertible. If $\det(A) = 0$, $A$ is singular (non-invertible).
-   **Rank**: The rank of a matrix is the maximum number of linearly independent rows or columns, indicating the dimension of the column space.

## Taylor Expansion

The Taylor expansion approximates functions using polynomials, which is especially useful in economics for simplifying complex functions near a specific point. For a function $f(x)$, the Taylor series centered at $x = a$ is:

$$
f(x) \approx f(a) + f'(a)(x - a) + \frac{f''(a)}{2!}(x - a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x - a)^n
$$

### Special Case: Maclaurin Series

When the expansion is centered at $a = 0$, the series is called a Maclaurin series: $$
f(x) \approx f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + \cdots + \frac{f^{(n)}(0)}{n!}x^n
$$

### Examples

1.  **Taylor Expansion of** $e^x$ around $x = 0$ (Maclaurin Series)
    -   Since $f(x) = e^x$ has derivatives that are all equal to $e^x$, the Taylor expansion is: $$
        e^x \approx 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
        $$ This series is often used in finance and economics for compounding approximations.
2.  **Taylor Expansion of** $\ln(1 + x)$ around $x = 0$
    -   For $f(x) = \ln(1 + x)$, we calculate the derivatives at $x = 0$:
        -   $f(0) = 0$
        -   $f'(0) = 1$
        -   $f''(0) = -1$
        -   $f'''(0) = 2$, and so forth.
    -   The series becomes: $$
        \ln(1 + x) \approx x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \cdots
        $$ This expansion is useful for approximating log functions in elasticity and utility models.

### Applications

Taylor expansions simplify complex functions, making them more manageable in linear approximations for economic modeling, optimization, and sensitivity analysis.

## Integration by Parts

Integration by parts is a technique for integrating the product of two functions. It’s especially useful in economics when dealing with functions like demand or production functions. The formula for integration by parts is:

$$
\int u \, dv = uv - \int v \, du
$$

where $u$ and $dv$ are chosen from the original integrand.

### Steps for Applying Integration by Parts

1.  Identify parts of the function to set as $u$ and $dv$.
2.  Differentiate $u$ to find $du$ and integrate $dv$ to find $v$.
3.  Substitute into the formula and simplify.

### Examples

1.  **Example 1:** $\int x e^x \, dx$
    -   Set $u = x$ (so $du = dx$) and $dv = e^x \, dx$ (so $v = e^x$).
    -   Applying the formula: $$
        \int x e^x \, dx = x e^x - \int e^x \, dx = x e^x - e^x + C = e^x(x - 1) + C
        $$
2.  **Example 2:** $\int x \ln(x) \, dx$
    -   Set $u = \ln(x)$ (so $du = \frac{1}{x} \, dx$) and $dv = x \, dx$ (so $v = \frac{x^2}{2}$).
    -   Applying the formula: $$
        \int x \ln(x) \, dx = \frac{x^2}{2} \ln(x) - \int \frac{x^2}{2} \cdot \frac{1}{x} \, dx = \frac{x^2}{2} \ln(x) - \int \frac{x}{2} \, dx
        $$
    -   Integrating the remaining term: $$
        = \frac{x^2}{2} \ln(x) - \frac{x^2}{4} + C
        $$
3.  **Example 3:** $\int x \cos(x) \, dx$
    -   Set $u = x$ (so $du = dx$) and $dv = \cos(x) \, dx$ (so $v = \sin(x)$).
    -   Applying the formula: $$
        \int x \cos(x) \, dx = x \sin(x) - \int \sin(x) \, dx = x \sin(x) + \cos(x) + C
        $$

### Application

Integration by parts is commonly applied in economics when dealing with elasticity, consumer surplus, or certain probability functions. It helps decompose complex integrals into manageable parts.
