---
title: "linear regression"
---

## Linear Regression

Linear regression is a fundamental statistical method for estimating the relationship between a dependent variable and one or more independent variables.

### 1. Population Regression Function

The **population regression function** describes the expected value of the outcome $Y$ given the value of $X$:

$$
E[Y | X] = \beta_0 + \beta_1 X
$$

For a binary variable $X$, this function indicates the difference in $Y$ between two groups: - $E[Y | X = 1] - E[Y | X = 0] = \beta_1$

For multiple variables (e.g., education $X_1$ and experience $X_2$), the function becomes:

$$
E[Y | X] = \beta_0 + \beta_1 X_1 + \beta_2 X_2
$$

### 2. Law of Iterated Expectations

The **Law of Iterated Expectations** states:

$$
E[Y] = E[E[Y | X]]
$$

For binary $X$, it simplifies to:

$$
E[Y] = E[Y | X = 1] \cdot P(X = 1) + E[Y | X = 0] \cdot P(X = 0)
$$ Where $P$ is a proportion.

### 3. Linear Regression Model with Error Term

The regression model includes an error term $u$ to capture the variation in $Y$ unexplained by $X$:

$$
Y = \beta_0 + \beta_1 X + u
$$

where $E[u | X] = 0$. The full model for multiple variables $X_1, X_2, \ldots, X_k$ is:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k + u
$$

### 4. Ordinary Least Squares (OLS) Estimation

The OLS estimator minimizes the sum of squared residuals (differences between observed and predicted values):

$$
\hat{\beta} = \text{argmin}_\beta \sum_{i=1}^n (Y_i - X_i' \beta)^2
$$

where $X_i'$ is the vector of regressors for observation $i$. The solution for $\hat{\beta}$, the OLS estimator, is:

$$
\hat{\beta} = (X'X)^{-1} X'Y
$$

#### Example (Bivariate Regression)

For a simple linear model with one regressor:

$$
Y = \beta_0 + \beta_1 X + u
$$

The slope coefficient $\beta_1$ is:

$$
\beta_1 = \frac{\text{cov}(Y, X)}{\text{var}(X)}
$$

and the intercept $\beta_0$ is:

$$
\beta_0 = E[Y] - \beta_1 E[X]
$$

### 5. Fitted Values and Residuals

-   **Fitted values**: $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$
-   **Residuals**: $\hat{u}_i = Y_i - \hat{Y}_i$

### 6. Properties of the OLS Estimator

1.  **Unbiasedness**: Under the assumption $E[u | X] = 0$, the OLS estimator is unbiased: $$
    E[\hat{\beta}] = \beta
    $$
2.  **Consistency**: As the sample size $n \to \infty$, $\hat{\beta}$ converges to the true $\beta$.

### 7. Variance of the OLS Estimator

If $u$ has constant variance (homoskedasticity), the variance of $\hat{\beta}$ is:

$$
\text{var}(\hat{\beta}) = \sigma^2 (X'X)^{-1}
$$

where $\sigma^2 = \text{var}(u)$. Under heteroskedasticity, robust standard errors are often used instead.

### 8. Hypothesis Testing in Linear Regression

To test if a coefficient $\beta_j = 0$, we calculate the t-statistic:

$$
t = \frac{\hat{\beta}_j}{\text{se}(\hat{\beta}_j)}
$$

where $\text{se}(\hat{\beta}_j)$ is the standard error of $\hat{\beta}_j$. For a 5% significance level, if $|t| > 1.96$, we reject the null hypothesis $\beta_j = 0$.

### 9. R-Squared and Goodness of Fit

-   **R-Squared**: Measures the proportion of variance in $Y$ explained by $X$. $$
    R^2 = 1 - \frac{\sum_{i=1}^n \hat{u}_i^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2}
    $$
-   **Interpretation**: An $R^2$ close to 1 suggests a strong relationship between $Y$ and $X$, while an $R^2$ close to 0 suggests a weak relationship.

### 10. Multicollinearity

Multicollinearity occurs when predictors in $X$ are highly correlated, making it difficult to isolate the effect of each predictor on $Y$. This can cause large variances in $\hat{\beta}$, leading to unreliable estimates.

### 11. Large Sample Distribution of the OLS Estimator

For large samples, $\hat{\beta}$ is approximately normally distributed:

$$
\sqrt{n}(\hat{\beta} - \beta) \sim N(0, V)
$$

where $V = Q^{-1} \Sigma Q^{-1}$, with $Q = E[X'X]$ and $\Sigma = E[u^2 X X']$.

## Deriving the argmin Function in Linear Regression

In the context of Ordinary Least Squares (OLS) regression, the **argmin** function identifies the values of the coefficients $\beta = (\beta_0, \beta_1, \ldots, \beta_k)$ that minimize the average squared difference between the observed values and the values predicted by the model.

### Step 1: Setting up the Problem

To find the best linear approximation of $Y$ in terms of $X$, we express $Y$ as:

$$
Y = X'\beta + u
$$

where: - $Y$ is the dependent variable. - $X$ is a matrix of independent variables (each row represents an observation, and each column represents a variable). - $\beta$ is the vector of coefficients we aim to estimate. - $u$ is the error term, assumed to have a mean of zero, i.e., $E[u | X] = 0$.

Given this setup, we seek the $\beta$ that minimizes the expected squared error, or the mean squared error function:

$$
\beta = \text{argmin}_{b \in \mathbb{R}^k} E[(Y - X'b)^2]
$$

### Step 2: Using the Property of Expectations to Define the Objective

For any value $c$, we know that:

$$
E[(Y - E[Y])^2] \leq E[(Y - c)^2]
$$

This property implies that the optimal $b$ minimizes the squared deviation of $Y$ from its conditional expectation $E[Y | X]$. Hence, $\beta$ is the value that minimizes:

$$
E[(Y - X'b)^2]
$$

This function is minimized when $E[Y | X] = X'\beta$, which forms the basis of the OLS estimation.

### Step 3: Sample-Based Minimization (Estimation with Observed Data)

In practice, we estimate $\beta$ using a sample of data points $(Y_1, X_1), (Y_2, X_2), \ldots, (Y_n, X_n)$. The sample-based version of the minimization problem becomes:

$$
\hat{\beta} = \text{argmin}_{b \in \mathbb{R}^k} \frac{1}{n} \sum_{i=1}^n (Y_i - X_i'b)^2
$$

Alternatively, this can be simplified as:

$$
\hat{\beta} = \text{argmin}_{b \in \mathbb{R}^k} \sum_{i=1}^n (Y_i - X_i'b)^2
$$

### Step 4: Solving for $\hat{\beta}$

To find the values of $\hat{\beta}$ that minimize the sum of squared residuals, we differentiate the objective function with respect to each element of $\beta$, set these derivatives to zero, and solve for $\beta$. This yields the OLS estimator:

$$
\hat{\beta} = (X'X)^{-1} X'Y
$$

This solution for $\hat{\beta}$ provides the regression coefficients that minimize the sum of squared residuals, making it the best linear predictor of $Y$ based on $X$.
