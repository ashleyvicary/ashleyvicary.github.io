[
  {
    "objectID": "math-tools.html",
    "href": "math-tools.html",
    "title": "math tools",
    "section": "",
    "text": "Derivatives are essential for understanding rates of change, especially in economics for optimizing functions like cost, revenue, and utility. Here are the core rules:\n\nPower Rule: If \\(f(x) = x^n\\), then \\(f'(x) = n \\cdot x^{n-1}\\).\nConstant Rule: If \\(f(x) = c\\) (where \\(c\\) is a constant), then \\(f'(x) = 0\\).\nProduct Rule: If \\(f(x) = g(x) \\cdot h(x)\\), then \\(f'(x) = g(x) \\cdot h'(x) + h(x) \\cdot g'(x)\\).\nQuotient Rule: If \\(f(x) = \\frac{g(x)}{h(x)}\\), then \\(f'(x) = \\frac{g'(x) \\cdot h(x) - g(x) \\cdot h'(x)}{[h(x)]^2}\\).\nChain Rule: If \\(f(x) = g(h(x))\\), then \\(f'(x) = g'(h(x)) \\cdot h'(x)\\).\n\n\n\n\nExponential Function: If \\(f(x) = e^x\\), then \\(f'(x) = e^x\\). If \\(f(x) = e^{2x}\\), then \\(f'(x) = 2e^{2x}\\).\nLogarithmic Function: If \\(f(x) = \\ln(x)\\), then \\(f'(x) = \\frac{1}{x}\\).\n\nThese rules provide the foundation for differentiating complex functions and are widely used in economic analysis, from marginal cost calculations to elasticity evaluations."
  },
  {
    "objectID": "math-tools.html#derivative-rules",
    "href": "math-tools.html#derivative-rules",
    "title": "math tools",
    "section": "",
    "text": "Derivatives are essential for understanding rates of change, especially in economics for optimizing functions like cost, revenue, and utility. Here are the core rules:\n\nPower Rule: If \\(f(x) = x^n\\), then \\(f'(x) = n \\cdot x^{n-1}\\).\nConstant Rule: If \\(f(x) = c\\) (where \\(c\\) is a constant), then \\(f'(x) = 0\\).\nProduct Rule: If \\(f(x) = g(x) \\cdot h(x)\\), then \\(f'(x) = g(x) \\cdot h'(x) + h(x) \\cdot g'(x)\\).\nQuotient Rule: If \\(f(x) = \\frac{g(x)}{h(x)}\\), then \\(f'(x) = \\frac{g'(x) \\cdot h(x) - g(x) \\cdot h'(x)}{[h(x)]^2}\\).\nChain Rule: If \\(f(x) = g(h(x))\\), then \\(f'(x) = g'(h(x)) \\cdot h'(x)\\).\n\n\n\n\nExponential Function: If \\(f(x) = e^x\\), then \\(f'(x) = e^x\\). If \\(f(x) = e^{2x}\\), then \\(f'(x) = 2e^{2x}\\).\nLogarithmic Function: If \\(f(x) = \\ln(x)\\), then \\(f'(x) = \\frac{1}{x}\\).\n\nThese rules provide the foundation for differentiating complex functions and are widely used in economic analysis, from marginal cost calculations to elasticity evaluations."
  },
  {
    "objectID": "math-tools.html#exponent-rules",
    "href": "math-tools.html#exponent-rules",
    "title": "math tools",
    "section": "Exponent Rules",
    "text": "Exponent Rules\nExponential rules are useful for simplifying functions in economic analysis, particularly in growth models and elasticity calculations. Here are the fundamental rules:\n\nProduct of Powers: \\(a^m \\cdot a^n = a^{m+n}\\)\nQuotient of Powers: \\(\\frac{a^m}{a^n} = a^{m-n}\\) (where \\(a \\neq 0\\))\nPower of a Power: \\((a^m)^n = a^{m \\cdot n}\\)\nPower of a Product: \\((a \\cdot b)^n = a^n \\cdot b^n\\)\nPower of a Quotient: \\(\\left(\\frac{a}{b}\\right)^n = \\frac{a^n}{b^n}\\) (where \\(b \\neq 0\\))\n\n\nSpecial Exponent Cases\n\nZero Exponent: \\(a^0 = 1\\) (where \\(a \\neq 0\\))\nNegative Exponent: \\(a^{-n} = \\frac{1}{a^n}\\) (where \\(a \\neq 0\\))\nFractional Exponent: \\(a^{\\frac{m}{n}} = \\sqrt[n]{a^m}\\)\n\nThese rules help simplify complex expressions, especially in compound interest calculations and growth models."
  },
  {
    "objectID": "math-tools.html#logarithmic-laws",
    "href": "math-tools.html#logarithmic-laws",
    "title": "math tools",
    "section": "Logarithmic Laws",
    "text": "Logarithmic Laws\nLogarithmic functions are essential tools in economics, especially for transforming data and simplifying multiplicative relationships. Here are the key laws:\n\nProduct Rule: \\(\\log_b(xy) = \\log_b(x) + \\log_b(y)\\)\nQuotient Rule: \\(\\log_b\\left(\\frac{x}{y}\\right) = \\log_b(x) - \\log_b(y)\\)\nPower Rule: \\(\\log_b(x^k) = k \\cdot \\log_b(x)\\)\nChange of Base Formula: \\(\\log_b(x) = \\frac{\\log_k(x)}{\\log_k(b)}\\) (useful for switching bases)\n\n\nSpecial Logarithmic Properties\n\nLog of 1: \\(\\log_b(1) = 0\\)\nLog of the Base: \\(\\log_b(b) = 1\\)\nInverse Property: \\(b^{\\log_b(x)} = x\\)"
  },
  {
    "objectID": "math-tools.html#unconstrained-optimization",
    "href": "math-tools.html#unconstrained-optimization",
    "title": "math tools",
    "section": "Unconstrained Optimization",
    "text": "Unconstrained Optimization\n\nSingle Variable\n\nCritical Point: For functions like a profit function \\(π(q)\\), find the critical point by setting the first derivative (FOC) to zero. Example: ( \\(\\frac{d\\pi(q)}{dq} = 0\\)).\nSecond Order Condition: Use the second derivative to determine if it’s a max or min. If (\\(\\frac{d^2 \\pi}{dq^2} &lt; 0\\)), it’s a local max.\n\n\n\nMultiple Variables\n\nPartial Derivatives: For multivariable functions, set partial derivatives with respect to each variable to zero to find critical points.\nHessian Matrix: For max/min, check if the Hessian matrix is positive or negative definite."
  },
  {
    "objectID": "math-tools.html#implicit-function-theorem-envelope-theorem",
    "href": "math-tools.html#implicit-function-theorem-envelope-theorem",
    "title": "math tools",
    "section": "Implicit Function Theorem & Envelope Theorem",
    "text": "Implicit Function Theorem & Envelope Theorem\n\nImplicit Function Theorem: Determines how a function changes with parameters by simplifying derivatives.\nEnvelope Theorem: For optimized functions, it simplifies the derivative calculation to focus on the direct effect."
  },
  {
    "objectID": "math-tools.html#constrained-optimization",
    "href": "math-tools.html#constrained-optimization",
    "title": "math tools",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\n\nLagrange Method: For maximizing under constraints, use Lagrangian ( \\(L = f(x) + \\lambda g(x)\\) ) with FOCs.\nExample: Maximize area of a rectangular fence, given perimeter constraint ( \\(2x + 2y = p\\) )."
  },
  {
    "objectID": "math-tools.html#linear-algebra",
    "href": "math-tools.html#linear-algebra",
    "title": "math tools",
    "section": "Linear Algebra",
    "text": "Linear Algebra\nLinear algebra is foundational for economic modeling, especially in optimization, econometrics, and data analysis. Here are some essential concepts:\n\nVectors and Matrices\n\nVector: An ordered list of numbers, typically denoted as \\(v = [v_1, v_2, \\dots, v_n]\\).\nMatrix: A rectangular array of numbers with dimensions \\(m \\times n\\), typically written as \\(A = [a_{ij}]\\), where \\(a_{ij}\\) represents the element in the \\(i^{th}\\) row and \\(j^{th}\\) column.\n\n\n\nMatrix Operations\n\nAddition and Subtraction: Matrices of the same dimensions can be added or subtracted element-wise. If \\(A\\) and \\(B\\) are \\(m \\times n\\) matrices, then \\((A + B)_{ij} = a_{ij} + b_{ij}\\).\nExample: \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\), \\(B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\), then \\(A + B = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\\).\nScalar Multiplication: Multiplying a matrix by a scalar \\(c\\) scales each element by \\(c\\), i.e., \\((cA)_{ij} = c \\cdot a_{ij}\\).\nExample: \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\), then \\(2A = \\begin{bmatrix} 2 & 4 \\\\ 6 & 8 \\end{bmatrix}\\).\nMatrix Multiplication: For matrices \\(A\\) of dimensions \\(m \\times n\\) and \\(B\\) of dimensions \\(n \\times p\\), the product \\(AB\\) is an \\(m \\times p\\) matrix where \\((AB)_{ij} = \\sum_{k=1}^{n} a_{ik} \\cdot b_{kj}\\).\nExample: \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\), \\(B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\), then \\(AB = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\\).\n\n\n\nSpecial Matrices\n\nIdentity Matrix: A square matrix with ones on the diagonal and zeros elsewhere, denoted \\(I\\). For any matrix \\(A\\), \\(AI = IA = A\\).\nExample: \\(I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\).\nTranspose: The transpose of a matrix \\(A\\), denoted \\(A^T\\), swaps its rows and columns.\nExample: If \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\), then \\(A^T = \\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\end{bmatrix}\\).\nInverse: For a square matrix \\(A\\), the inverse \\(A^{-1}\\) (if it exists) satisfies \\(A \\cdot A^{-1} = I\\).\nExample: If \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\), then \\(A^{-1} = \\begin{bmatrix} -2 & 1 \\\\ 1.5 & -0.5 \\end{bmatrix}\\).\n\n\n\nDeterminants and Rank\n\nDeterminant: A scalar value associated with a square matrix, denoted \\(\\det(A)\\) or \\(|A|\\), which indicates whether a matrix is invertible. If \\(\\det(A) = 0\\), \\(A\\) is singular (non-invertible).\nRank: The rank of a matrix is the maximum number of linearly independent rows or columns, indicating the dimension of the column space."
  },
  {
    "objectID": "math-tools.html#taylor-expansion",
    "href": "math-tools.html#taylor-expansion",
    "title": "math tools",
    "section": "Taylor Expansion",
    "text": "Taylor Expansion\nThe Taylor expansion approximates functions using polynomials, which is especially useful in economics for simplifying complex functions near a specific point. For a function \\(f(x)\\), the Taylor series centered at \\(x = a\\) is:\n\\[\nf(x) \\approx f(a) + f'(a)(x - a) + \\frac{f''(a)}{2!}(x - a)^2 + \\cdots + \\frac{f^{(n)}(a)}{n!}(x - a)^n\n\\]\n\nSpecial Case: Maclaurin Series\nWhen the expansion is centered at \\(a = 0\\), the series is called a Maclaurin series: \\[\nf(x) \\approx f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\cdots + \\frac{f^{(n)}(0)}{n!}x^n\n\\]\n\n\nExamples\n\nTaylor Expansion of \\(e^x\\) around \\(x = 0\\) (Maclaurin Series)\n\nSince \\(f(x) = e^x\\) has derivatives that are all equal to \\(e^x\\), the Taylor expansion is: \\[\ne^x \\approx 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots\n\\] This series is often used in finance and economics for compounding approximations.\n\nTaylor Expansion of \\(\\ln(1 + x)\\) around \\(x = 0\\)\n\nFor \\(f(x) = \\ln(1 + x)\\), we calculate the derivatives at \\(x = 0\\):\n\n\\(f(0) = 0\\)\n\\(f'(0) = 1\\)\n\\(f''(0) = -1\\)\n\\(f'''(0) = 2\\), and so forth.\n\nThe series becomes: \\[\n\\ln(1 + x) \\approx x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\cdots\n\\] This expansion is useful for approximating log functions in elasticity and utility models.\n\n\n\n\nApplications\nTaylor expansions simplify complex functions, making them more manageable in linear approximations for economic modeling, optimization, and sensitivity analysis."
  },
  {
    "objectID": "math-tools.html#integration-by-parts",
    "href": "math-tools.html#integration-by-parts",
    "title": "math tools",
    "section": "Integration by Parts",
    "text": "Integration by Parts\nIntegration by parts is a technique for integrating the product of two functions. It’s especially useful in economics when dealing with functions like demand or production functions. The formula for integration by parts is:\n\\[\n\\int u \\, dv = uv - \\int v \\, du\n\\]\nwhere \\(u\\) and \\(dv\\) are chosen from the original integrand.\n\nSteps for Applying Integration by Parts\n\nIdentify parts of the function to set as \\(u\\) and \\(dv\\).\nDifferentiate \\(u\\) to find \\(du\\) and integrate \\(dv\\) to find \\(v\\).\nSubstitute into the formula and simplify.\n\n\n\nExamples\n\nExample 1: \\(\\int x e^x \\, dx\\)\n\nSet \\(u = x\\) (so \\(du = dx\\)) and \\(dv = e^x \\, dx\\) (so \\(v = e^x\\)).\nApplying the formula: \\[\n\\int x e^x \\, dx = x e^x - \\int e^x \\, dx = x e^x - e^x + C = e^x(x - 1) + C\n\\]\n\nExample 2: \\(\\int x \\ln(x) \\, dx\\)\n\nSet \\(u = \\ln(x)\\) (so \\(du = \\frac{1}{x} \\, dx\\)) and \\(dv = x \\, dx\\) (so \\(v = \\frac{x^2}{2}\\)).\nApplying the formula: \\[\n\\int x \\ln(x) \\, dx = \\frac{x^2}{2} \\ln(x) - \\int \\frac{x^2}{2} \\cdot \\frac{1}{x} \\, dx = \\frac{x^2}{2} \\ln(x) - \\int \\frac{x}{2} \\, dx\n\\]\nIntegrating the remaining term: \\[\n= \\frac{x^2}{2} \\ln(x) - \\frac{x^2}{4} + C\n\\]\n\nExample 3: \\(\\int x \\cos(x) \\, dx\\)\n\nSet \\(u = x\\) (so \\(du = dx\\)) and \\(dv = \\cos(x) \\, dx\\) (so \\(v = \\sin(x)\\)).\nApplying the formula: \\[\n\\int x \\cos(x) \\, dx = x \\sin(x) - \\int \\sin(x) \\, dx = x \\sin(x) + \\cos(x) + C\n\\]\n\n\n\n\nApplication\nIntegration by parts is commonly applied in economics when dealing with elasticity, consumer surplus, or certain probability functions. It helps decompose complex integrals into manageable parts."
  },
  {
    "objectID": "causal-inference.html",
    "href": "causal-inference.html",
    "title": "causal inference",
    "section": "",
    "text": "Causal inference is central to understanding the effects of policies and interventions. Unlike correlation, which only shows relationships between variables, causal inference aims to uncover whether one variable causes changes in another.\n\n\n\nCausal Effect: The effect of a treatment (like a policy change) on an outcome.\nRandomized Controlled Trials (RCTs): The gold standard for causal inference. Randomization distributes all confounding factors evenly, allowing clear estimation of causal effects.\nQuasi-Experiments: Situations where randomization is not feasible, but events (like policy changes) create conditions similar to an RCT. Example: Studying wealth effects by comparing lottery winners to non-winners.\n\n\n\n\nFor each subject, we observe only one outcome—either treated or untreated—not both. This makes it impossible to directly observe the causal effect on that individual.\n\nCounterfactuals: We define causal effects relative to hypothetical alternatives (what would happen with and without treatment).\nTreatment Effect: \\(T = Y_1 - Y_0\\), where \\(Y_1\\) is the outcome with treatment, and \\(Y_0\\) without it.\n\n\n\n\n\nRandomization: Distributes potential confounders evenly across groups.\nDifference-in-Differences (DiD): Compares changes over time between treated and control groups to estimate causal effects.\nInstrumental Variables (IV): Uses variables that influence the treatment but not the outcome, isolating the causal effect.\nRegression Discontinuity (RD): Compares outcomes near a threshold to estimate causal effects.\n\n\n\n\nIn observational studies, simply comparing treated (\\(X = 1\\)) and untreated (\\(X = 0\\)) groups may introduce bias:\n\\[\n\\tilde{T} = E[Y | X = 1] - E[Y | X = 0] = T + \\{E[Y_0 | X = 1] - E[Y_0 | X = 0]\\}\n\\]\nThe term \\(\\{E[Y_0 | X = 1] - E[Y_0 | X = 0]\\}\\) represents bias—it occurs when treated and untreated groups differ in ways that affect the outcome independently of the treatment. Bias may lead to over- or underestimation of the true causal effect \\(T\\)."
  },
  {
    "objectID": "causal-inference.html#introduction",
    "href": "causal-inference.html#introduction",
    "title": "causal inference",
    "section": "",
    "text": "Causal inference is central to understanding the effects of policies and interventions. Unlike correlation, which only shows relationships between variables, causal inference aims to uncover whether one variable causes changes in another.\n\n\n\nCausal Effect: The effect of a treatment (like a policy change) on an outcome.\nRandomized Controlled Trials (RCTs): The gold standard for causal inference. Randomization distributes all confounding factors evenly, allowing clear estimation of causal effects.\nQuasi-Experiments: Situations where randomization is not feasible, but events (like policy changes) create conditions similar to an RCT. Example: Studying wealth effects by comparing lottery winners to non-winners.\n\n\n\n\nFor each subject, we observe only one outcome—either treated or untreated—not both. This makes it impossible to directly observe the causal effect on that individual.\n\nCounterfactuals: We define causal effects relative to hypothetical alternatives (what would happen with and without treatment).\nTreatment Effect: \\(T = Y_1 - Y_0\\), where \\(Y_1\\) is the outcome with treatment, and \\(Y_0\\) without it.\n\n\n\n\n\nRandomization: Distributes potential confounders evenly across groups.\nDifference-in-Differences (DiD): Compares changes over time between treated and control groups to estimate causal effects.\nInstrumental Variables (IV): Uses variables that influence the treatment but not the outcome, isolating the causal effect.\nRegression Discontinuity (RD): Compares outcomes near a threshold to estimate causal effects.\n\n\n\n\nIn observational studies, simply comparing treated (\\(X = 1\\)) and untreated (\\(X = 0\\)) groups may introduce bias:\n\\[\n\\tilde{T} = E[Y | X = 1] - E[Y | X = 0] = T + \\{E[Y_0 | X = 1] - E[Y_0 | X = 0]\\}\n\\]\nThe term \\(\\{E[Y_0 | X = 1] - E[Y_0 | X = 0]\\}\\) represents bias—it occurs when treated and untreated groups differ in ways that affect the outcome independently of the treatment. Bias may lead to over- or underestimation of the true causal effect \\(T\\)."
  },
  {
    "objectID": "causal-inference.html#regression-discontinuity-rd",
    "href": "causal-inference.html#regression-discontinuity-rd",
    "title": "causal inference",
    "section": "Regression Discontinuity (RD)",
    "text": "Regression Discontinuity (RD)\nThe Regression Discontinuity (RD) design is a method for identifying causal effects when treatment assignment is based on a threshold in a continuous variable. It’s widely used in economics to evaluate policies, such as eligibility rules or cutoff points.\n\nKey Concepts\n\nAssignment Variable: A continuous variable that determines treatment eligibility at a specific cutoff (e.g., test scores, income level).\nThreshold/Cutoff: The point at which treatment is assigned. Units just above and below the cutoff provide a natural experiment for causal inference.\nTreatment and Control Groups: Units above the cutoff receive the treatment, while those below do not. The RD design assumes units near the threshold are similar except for treatment.\n\n\n\nRD Estimator\nThe RD design compares outcomes just above and below the cutoff to estimate the treatment effect. Formally, let \\(Y\\) be the outcome and \\(X\\) the assignment variable with cutoff \\(c\\). The causal effect at the cutoff is:\n\\[\n\\tau_{RD} = \\lim_{x \\to c^+} E[Y | X = x] - \\lim_{x \\to c^-} E[Y | X = x]\n\\]\n\n\nTypes of RD Designs\n\nSharp RD: Treatment assignment is strictly determined by the cutoff, i.e., everyone above the cutoff receives treatment, and no one below it does.\nFuzzy RD: Treatment assignment increases at the cutoff, but not perfectly. This is common when there’s imperfect compliance with treatment rules.\n\n\n\nExample: Scholarship Eligibility and Academic Outcomes\nImagine a scholarship awarded to students with a test score above 85. In a sharp RD design, students with scores of 85+ receive the scholarship, while those with scores just below do not. The RD design estimates the causal effect of receiving the scholarship on academic outcomes by comparing students just above and below the 85 threshold.\n\n\nAssumptions and Validity\n\nContinuity Assumption: Units just above and below the cutoff are assumed to be similar, differing only in treatment status.\nNo Manipulation: The assignment variable should not be manipulable; individuals cannot precisely control their score to just surpass the cutoff.\n\n\n\nApplication in Economics\nRD designs are useful when randomized experiments aren’t feasible, providing a credible estimate of causal effects in cases like educational policy evaluations, welfare eligibility, and more."
  },
  {
    "objectID": "causal-inference.html#difference-in-differences-did",
    "href": "causal-inference.html#difference-in-differences-did",
    "title": "causal inference",
    "section": "Difference-in-Differences (DiD)",
    "text": "Difference-in-Differences (DiD)\nDifference-in-Differences (DiD) is a method for estimating causal effects by comparing the change in outcomes over time between treated and control groups. DiD is particularly useful when randomization isn’t feasible but we can observe the same groups before and after a treatment.\n\nKey Concepts\n\nPre- and Post-Treatment Comparisons: DiD compares the change in outcomes for the treated group with the change in outcomes for the control group.\nParallel Trends Assumption: The treated and control groups must have followed similar trends in the absence of treatment.\n\n\n\nDiD Estimator\nThe DiD estimator calculates the treatment effect as:\n\\[\n\\Delta Y_{DiD} = (Y_{after}^{treated} - Y_{before}^{treated}) - (Y_{after}^{control} - Y_{before}^{control})\n\\]\n\n\nExample: Minimum Wage and Employment\nCard and Krueger’s (1994) study on the impact of minimum wage increases used DiD by comparing fast-food employment changes in New Jersey (where minimum wage increased) and Pennsylvania (where it did not). They found an increase in employment in New Jersey relative to Pennsylvania, challenging traditional predictions."
  },
  {
    "objectID": "causal-inference.html#instrumental-variables-iv",
    "href": "causal-inference.html#instrumental-variables-iv",
    "title": "causal inference",
    "section": "Instrumental Variables (IV)",
    "text": "Instrumental Variables (IV)\nInstrumental Variables (IV) is a technique used to estimate causal effects when there’s endogeneity—when an explanatory variable is correlated with the error term. An IV helps isolate the exogenous variation in the endogenous variable.\n\nKey Concepts\n\nInstrument: A variable that affects the endogenous variable (treatment) but has no direct effect on the outcome except through this treatment.\nExclusion Restriction: The instrument affects the outcome only through the treatment, not directly.\n\n\n\nIV Estimator\nIf \\(Z\\) is the instrument for \\(X\\), and \\(Y\\) is the outcome, the IV estimator (two-stage least squares) can be calculated as:\n\nFirst Stage: \\(X = \\alpha + \\beta Z + \\epsilon\\)\nSecond Stage: \\(Y = \\gamma + \\delta \\hat{X} + \\eta\\)\n\n\n\nExample: Education and Earnings\nTo estimate the causal effect of education on earnings, distance to college might serve as an instrument. Distance affects the likelihood of attending college (the treatment) but doesn’t directly affect earnings, isolating the causal impact of education."
  },
  {
    "objectID": "causal-inference.html#randomized-controlled-trials-rcts",
    "href": "causal-inference.html#randomized-controlled-trials-rcts",
    "title": "causal inference",
    "section": "Randomized Controlled Trials (RCTs)",
    "text": "Randomized Controlled Trials (RCTs)\nRandomized Controlled Trials (RCTs) are the gold standard in causal inference, where random assignment to treatment and control groups ensures that any differences in outcomes can be attributed to the treatment.\n\nKey Concepts\n\nRandom Assignment: Units are randomly assigned to treatment and control groups, balancing confounders across groups.\nControl Group: Provides a baseline for comparison to the treatment group.\n\n\n\nRCT Estimator\nThe average treatment effect in an RCT is calculated as:\n\\[\nATE = E[Y | X = 1] - E[Y | X = 0]\n\\]\nwhere \\(X = 1\\) indicates the treatment group and \\(X = 0\\) the control group.\n\n\nExample: Drug Trials\nIn a clinical trial, participants are randomly assigned to receive either a new drug or a placebo. The difference in health outcomes between the groups estimates the causal effect of the drug, assuming random assignment balances all other factors."
  },
  {
    "objectID": "history-timeline.html",
    "href": "history-timeline.html",
    "title": "history timelines",
    "section": "",
    "text": "A timeline of key events in history according to me. To help with my Economist Dateline score (this is why it starts in 1843) ;)"
  },
  {
    "objectID": "history-timeline.html#us-presidents",
    "href": "history-timeline.html#us-presidents",
    "title": "history timelines",
    "section": "US Presidents",
    "text": "US Presidents"
  },
  {
    "objectID": "history-timeline.html#timeline-of-major-world-events-since-1843",
    "href": "history-timeline.html#timeline-of-major-world-events-since-1843",
    "title": "history timelines",
    "section": "Timeline of Major World Events (Since 1843)",
    "text": "Timeline of Major World Events (Since 1843)\n\n\n\n\n\n\n\nYear\nEvent\n\n\n\n\n1848\nRevolutions of 1848 across Europe advocating for democratic reforms.\n\n\n1853-1856\nCrimean War between Russia and an alliance of Britain, France, and the Ottoman Empire.\n\n\n1861-1865\nAmerican Civil War in the United States over issues of slavery and state sovereignty.\n\n\n1869\nTranscontinental Railroad completed in the U.S., connecting the East and West coasts.\n\n\n1882\nTriple Alliance formed between Germany, Austria-Hungary, and Italy, marking alliances that would lead to World War I.\n\n\n1885\nBerlin Conference divides Africa among European powers, intensifying colonialism.\n\n\n1914-1918\nWorld War I, a global war involving most world powers, ending with the Treaty of Versailles.\n\n\n1917\nRussian Revolution, leading to the rise of the Soviet Union.\n\n\n1929\nGreat Depression begins, causing worldwide economic hardship.\n\n\n1939-1945\nWorld War II, a global conflict involving the Allies and Axis powers, resulting in major changes to global politics.\n\n\n1945\nUnited Nations founded to promote international cooperation and prevent future wars.\n\n\n1947\nIndia gains independence from British rule, marking the end of the British Empire’s dominance in South Asia.\n\n\n1949\nNATO established as a military alliance among Western nations; People’s Republic of China founded under communist rule.\n\n\n1957\nLaunch of Sputnik by the Soviet Union, beginning the Space Race.\n\n\n1969\nFirst Moon Landing by Apollo 11, a major milestone in space exploration.\n\n\n1989\nFall of the Berlin Wall, signaling the end of the Cold War.\n\n\n1991\nDissolution of the Soviet Union, leading to the independence of several states and the end of the Cold War.\n\n\n2001\nSeptember 11 Attacks in the U.S., leading to global anti-terrorism efforts.\n\n\n2008\nGlobal Financial Crisis, affecting economies worldwide.\n\n\n2020\nCOVID-19 Pandemic begins, causing global health crises, economic impact, and social change."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "my general exam preparation",
    "section": "",
    "text": "the purpose of this site:\n\nhelp me prepare for my phd general exams\ndocument my research\nshare my research with others"
  },
  {
    "objectID": "what-is-economics.html",
    "href": "what-is-economics.html",
    "title": "what is economics",
    "section": "",
    "text": "economics is the study of how people allocate scarce resources to satisfy their unlimited wants.\n\nmicroeconomics\nmicroeconomics is the study of how individuals and firms make decisions and how these decisions interact.\n\n\nmacroeconomics\nmacroeconomics is the study of the economy as a whole. it is concerned with the behavior of the economy as a whole, including topics such as inflation, unemployment, and economic growth."
  },
  {
    "objectID": "history-stuff.html",
    "href": "history-stuff.html",
    "title": "history stuff",
    "section": "",
    "text": "Capitalism is an economic system where trade, industry, and the means of production are largely or entirely privately owned and operated for profit. In capitalism, capital (money or other assets) is invested with the aim of generating returns, with markets primarily guiding production and distribution."
  },
  {
    "objectID": "history-stuff.html#what-is-capitalism",
    "href": "history-stuff.html#what-is-capitalism",
    "title": "history stuff",
    "section": "",
    "text": "Capitalism is an economic system where trade, industry, and the means of production are largely or entirely privately owned and operated for profit. In capitalism, capital (money or other assets) is invested with the aim of generating returns, with markets primarily guiding production and distribution."
  },
  {
    "objectID": "history-stuff.html#before-capitalism",
    "href": "history-stuff.html#before-capitalism",
    "title": "history stuff",
    "section": "Before Capitalism",
    "text": "Before Capitalism\nBefore capitalism, most economies were structured around feudalism (up to the 15th century in Europe). Under feudalism, land was the main source of wealth, controlled by nobles, and labor was provided by serfs bound to the land. Production was for subsistence rather than profit, with little to no focus on market-based exchange."
  },
  {
    "objectID": "history-stuff.html#origins-and-timeline-of-capitalism",
    "href": "history-stuff.html#origins-and-timeline-of-capitalism",
    "title": "history stuff",
    "section": "Origins and Timeline of Capitalism",
    "text": "Origins and Timeline of Capitalism\n\nEarly 16th to 18th Century: Proto-capitalist structures began emerging with the Commercial Revolution in Europe. Trade expanded, and early banking and joint-stock companies were created, laying the groundwork for capitalism.\nLate 18th Century: Capitalism as we recognize it took root with the Industrial Revolution (starting around the 1760s in Britain). This period saw significant technological advancements and mechanization, leading to mass production and factory systems.\n1776: Adam Smith’s seminal work, The Wealth of Nations, was published, advocating for free markets and explaining productivity gains from the division of labor. Smith’s pin factory example demonstrated how dividing tasks increased output significantly."
  },
  {
    "objectID": "history-stuff.html#division-of-labor",
    "href": "history-stuff.html#division-of-labor",
    "title": "history stuff",
    "section": "Division of Labor",
    "text": "Division of Labor\nIn the pin factory example, Smith describes how dividing the production of pins into specialized tasks allowed workers to produce thousands of pins per day, compared to the few they could make individually. This concept became a foundational element of capitalist production, emphasizing efficiency and specialization to maximize profit."
  },
  {
    "objectID": "reading-list-by-topic.html",
    "href": "reading-list-by-topic.html",
    "title": "reading list",
    "section": "",
    "text": "Understanding Early Childhood Development and Its Importance"
  },
  {
    "objectID": "reading-list-by-topic.html#early-childhood-education",
    "href": "reading-list-by-topic.html#early-childhood-education",
    "title": "reading list",
    "section": "",
    "text": "Understanding Early Childhood Development and Its Importance"
  },
  {
    "objectID": "key-players.html",
    "href": "key-players.html",
    "title": "key players",
    "section": "",
    "text": "daron acemoglu\nsummary:\nDaron Acemoglu is a Turkish economist who is a professor at MIT. He is best known for his work on political economy and economic development. He has written several books, including “Why Nations Fail” and “Economic Origins of Dictatorship and Democracy.”\nkey contributions:\nAcemoglu has made several key contributions to the field of economics. He is known for his work on the role of institutions in economic development, and has argued that inclusive institutions are essential for long-term growth. He has also studied the relationship between political institutions and economic outcomes, and has shown that democratic institutions are associated with higher levels of economic development (?).\n\n\ndavid autor\nsummary:\nDavid Autor is an American economist who is a professor at MIT. He is best known for his work on labor economics and the impact of technology on the labor market. He has written several influential papers on the subject, including “The China Shock” and “Why Are There Still So Many Jobs?”\nkey contributions:\nAutor has made several key contributions to the field of economics. He is known for his work on the impact of technology on the labor market, and has shown that technological change has had a significant impact on the distribution of income. He has also studied the role of trade in shaping the labor market, and has shown that trade with China has had a negative impact on the employment prospects of low-skilled workers."
  },
  {
    "objectID": "ci-and-hypothesis-testing.html",
    "href": "ci-and-hypothesis-testing.html",
    "title": "confidence intervals and hypothesis testing",
    "section": "",
    "text": "Confidence intervals provide a range around a sample estimate, indicating where the true population parameter is likely to lie with a given confidence level.\n\n\nFor a large sample with sample mean \\(\\bar{X}\\), standard deviation \\(\\sigma\\), and sample size \\(n\\), the confidence interval for the population mean \\(\\mu\\) is:\n\\[\n\\bar{X} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n\\]\nwhere: - \\(z_{\\alpha/2}\\) is the critical value from the standard normal distribution (e.g., \\(1.96\\) for 95% confidence). - \\(\\frac{\\sigma}{\\sqrt{n}}\\) is the standard error of the mean.\n\n\n\n95% Confidence Interval: \\(\\bar{X} \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}\\)\n90% Confidence Interval: \\(\\bar{X} \\pm 1.64 \\frac{\\sigma}{\\sqrt{n}}\\)\n99% Confidence Interval: \\(\\bar{X} \\pm 2.58 \\frac{\\sigma}{\\sqrt{n}}\\)\n\nAs \\(n\\) increases, the interval becomes narrower, indicating greater precision in estimating \\(\\mu\\)."
  },
  {
    "objectID": "ci-and-hypothesis-testing.html#confidence-intervals",
    "href": "ci-and-hypothesis-testing.html#confidence-intervals",
    "title": "confidence intervals and hypothesis testing",
    "section": "",
    "text": "Confidence intervals provide a range around a sample estimate, indicating where the true population parameter is likely to lie with a given confidence level.\n\n\nFor a large sample with sample mean \\(\\bar{X}\\), standard deviation \\(\\sigma\\), and sample size \\(n\\), the confidence interval for the population mean \\(\\mu\\) is:\n\\[\n\\bar{X} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n\\]\nwhere: - \\(z_{\\alpha/2}\\) is the critical value from the standard normal distribution (e.g., \\(1.96\\) for 95% confidence). - \\(\\frac{\\sigma}{\\sqrt{n}}\\) is the standard error of the mean.\n\n\n\n95% Confidence Interval: \\(\\bar{X} \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}\\)\n90% Confidence Interval: \\(\\bar{X} \\pm 1.64 \\frac{\\sigma}{\\sqrt{n}}\\)\n99% Confidence Interval: \\(\\bar{X} \\pm 2.58 \\frac{\\sigma}{\\sqrt{n}}\\)\n\nAs \\(n\\) increases, the interval becomes narrower, indicating greater precision in estimating \\(\\mu\\)."
  },
  {
    "objectID": "ci-and-hypothesis-testing.html#hypothesis-testing",
    "href": "ci-and-hypothesis-testing.html#hypothesis-testing",
    "title": "confidence intervals and hypothesis testing",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nHypothesis testing assesses whether a sample provides sufficient evidence to reject a hypothesis about a population parameter.\n\nSteps in Hypothesis Testing\n\nSet Null and Alternative Hypotheses:\n\nNull Hypothesis \\(H_0\\): The parameter equals a specific value (e.g., \\(\\mu = m\\)).\nAlternative Hypothesis \\(H_1\\): The parameter differs from this value (e.g., \\(\\mu \\neq m\\)).\n\nCalculate the Test Statistic:\n\nFor large \\(n\\), the test statistic for the mean is: \\[\nt = \\frac{\\bar{X} - m}{\\text{s.e.}(\\bar{X})}\n\\] where \\(\\text{s.e.}(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\).\n\nDecision Rule:\n\nFor a 5% significance level, reject \\(H_0\\) if \\(|t| &gt; 1.96\\).\nAlternatively, if \\(p\\)-value &lt; 0.05, reject \\(H_0\\).\n\n\n\nCommon Significance Levels\n\n5% Level: Critical value of \\(|t| &gt; 1.96\\)\n1% Level: Critical value of \\(|t| &gt; 2.58\\)\n10% Level: Critical value of \\(|t| &gt; 1.64\\)"
  },
  {
    "objectID": "ci-and-hypothesis-testing.html#one-sample-test",
    "href": "ci-and-hypothesis-testing.html#one-sample-test",
    "title": "confidence intervals and hypothesis testing",
    "section": "One-Sample Test",
    "text": "One-Sample Test\nThe one-sample test compares the sample mean \\(\\bar{X}\\) to a hypothesized population mean \\(m\\).\n\nOne-Sample \\(t\\)-Test Statistic\nFor large \\(n\\), use:\n\\[\nt = \\frac{\\bar{X} - m}{\\text{s.e.}(\\bar{X})} = \\frac{\\bar{X} - m}{\\sigma / \\sqrt{n}}\n\\]\nwhere: - \\(\\bar{X}\\) is the sample mean. - \\(\\sigma\\) is the population standard deviation (or use sample standard deviation if unknown). - \\(n\\) is the sample size.\n\nDecision Rule\n\nReject \\(H_0\\) if \\(|t| &gt; 1.96\\) (for a 5% significance level).\n\n\n\nExample\nAssume \\(\\bar{X} = 50\\), \\(m = 52\\), \\(\\sigma = 10\\), and \\(n = 100\\). Calculate \\(t\\):\n\\[\nt = \\frac{50 - 52}{10 / \\sqrt{100}} = -2\n\\]\nSince \\(|t| = 2 &gt; 1.96\\), reject \\(H_0\\) at the 5% significance level."
  },
  {
    "objectID": "ci-and-hypothesis-testing.html#two-sample-test",
    "href": "ci-and-hypothesis-testing.html#two-sample-test",
    "title": "confidence intervals and hypothesis testing",
    "section": "Two-Sample Test",
    "text": "Two-Sample Test\nThe two-sample test compares the means of two independent samples to determine if they are statistically different.\n\nTwo-Sample \\(t\\)-Test Statistic\nFor two samples with means \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\), standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\), and sample sizes \\(n_1\\) and \\(n_2\\), the test statistic is:\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\\]\n\nHypotheses\n\nNull Hypothesis \\(H_0\\): \\(\\mu_1 = \\mu_2\\) (the two means are equal).\nAlternative Hypothesis \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\) (the means differ).\n\n\n\nDecision Rule\n\nReject \\(H_0\\) if \\(|t| &gt; 1.96\\) (for a 5% significance level).\n\n\n\nExample\nSuppose \\(\\bar{X}_1 = 0.0965\\), \\(\\bar{X}_2 = 0.0645\\), \\(n_1 = n_2 = 2435\\). Assume \\(X\\) is binary with variance \\(p(1 - p)\\), so:\n\\[\n\\sigma_1^2 = 0.0965 \\times (1 - 0.0965) / 2435\n\\] \\[\n\\sigma_2^2 = 0.0645 \\times (1 - 0.0645) / 2435\n\\]\nCalculate \\(t\\):\n\\[\nt = \\frac{0.0965 - 0.0645}{\\sqrt{0.0060 + 0.0050}} = 4.11\n\\]\nSince \\(|t| = 4.11 &gt; 1.96\\), reject \\(H_0\\) at the 5% significance level, concluding a significant difference between the two means."
  },
  {
    "objectID": "ci-and-hypothesis-testing.html#duality-between-confidence-intervals-and-hypothesis-tests",
    "href": "ci-and-hypothesis-testing.html#duality-between-confidence-intervals-and-hypothesis-tests",
    "title": "confidence intervals and hypothesis testing",
    "section": "Duality Between Confidence Intervals and Hypothesis Tests",
    "text": "Duality Between Confidence Intervals and Hypothesis Tests\nA value \\(m\\) is in a 95% confidence interval for \\(\\mu\\) if we do not reject the null hypothesis \\(H_0: \\mu = m\\) at the 5% level. If \\(m\\) is outside this interval, we reject \\(H_0\\).\nThis section covers essential tools for interpreting confidence intervals, one-sample and two-sample hypothesis tests, and the connection to significance testing."
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "linear regression",
    "section": "",
    "text": "Linear regression is a fundamental statistical method for estimating the relationship between a dependent variable and one or more independent variables.\n\n\nThe population regression function describes the expected value of the outcome \\(Y\\) given the value of \\(X\\):\n\\[\nE[Y | X] = \\beta_0 + \\beta_1 X\n\\]\nFor a binary variable \\(X\\), this function indicates the difference in \\(Y\\) between two groups: - \\(E[Y | X = 1] - E[Y | X = 0] = \\beta_1\\)\nFor multiple variables (e.g., education \\(X_1\\) and experience \\(X_2\\)), the function becomes:\n\\[\nE[Y | X] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\n\n\nThe Law of Iterated Expectations states:\n\\[\nE[Y] = E[E[Y | X]]\n\\]\nFor binary \\(X\\), it simplifies to:\n\\[\nE[Y] = E[Y | X = 1] \\cdot P(X = 1) + E[Y | X = 0] \\cdot P(X = 0)\n\\] Where \\(P\\) is a proportion.\n\n\n\nThe regression model includes an error term \\(u\\) to capture the variation in \\(Y\\) unexplained by \\(X\\):\n\\[\nY = \\beta_0 + \\beta_1 X + u\n\\]\nwhere \\(E[u | X] = 0\\). The full model for multiple variables \\(X_1, X_2, \\ldots, X_k\\) is:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + u\n\\]\n\n\n\nThe OLS estimator minimizes the sum of squared residuals (differences between observed and predicted values):\n\\[\n\\hat{\\beta} = \\text{argmin}_\\beta \\sum_{i=1}^n (Y_i - X_i' \\beta)^2\n\\]\nwhere \\(X_i'\\) is the vector of regressors for observation \\(i\\). The solution for \\(\\hat{\\beta}\\), the OLS estimator, is:\n\\[\n\\hat{\\beta} = (X'X)^{-1} X'Y\n\\]\n\n\nFor a simple linear model with one regressor:\n\\[\nY = \\beta_0 + \\beta_1 X + u\n\\]\nThe slope coefficient \\(\\beta_1\\) is:\n\\[\n\\beta_1 = \\frac{\\text{cov}(Y, X)}{\\text{var}(X)}\n\\]\nand the intercept \\(\\beta_0\\) is:\n\\[\n\\beta_0 = E[Y] - \\beta_1 E[X]\n\\]\n\n\n\n\n\nFitted values: \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\)\nResiduals: \\(\\hat{u}_i = Y_i - \\hat{Y}_i\\)\n\n\n\n\n\nUnbiasedness: Under the assumption \\(E[u | X] = 0\\), the OLS estimator is unbiased: \\[\nE[\\hat{\\beta}] = \\beta\n\\]\nConsistency: As the sample size \\(n \\to \\infty\\), \\(\\hat{\\beta}\\) converges to the true \\(\\beta\\).\n\n\n\n\nIf \\(u\\) has constant variance (homoskedasticity), the variance of \\(\\hat{\\beta}\\) is:\n\\[\n\\text{var}(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}\n\\]\nwhere \\(\\sigma^2 = \\text{var}(u)\\). Under heteroskedasticity, robust standard errors are often used instead.\n\n\n\nTo test if a coefficient \\(\\beta_j = 0\\), we calculate the t-statistic:\n\\[\nt = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)}\n\\]\nwhere \\(\\text{se}(\\hat{\\beta}_j)\\) is the standard error of \\(\\hat{\\beta}_j\\). For a 5% significance level, if \\(|t| &gt; 1.96\\), we reject the null hypothesis \\(\\beta_j = 0\\).\n\n\n\n\nR-Squared: Measures the proportion of variance in \\(Y\\) explained by \\(X\\). \\[\nR^2 = 1 - \\frac{\\sum_{i=1}^n \\hat{u}_i^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}\n\\]\nInterpretation: An \\(R^2\\) close to 1 suggests a strong relationship between \\(Y\\) and \\(X\\), while an \\(R^2\\) close to 0 suggests a weak relationship.\n\n\n\n\nMulticollinearity occurs when predictors in \\(X\\) are highly correlated, making it difficult to isolate the effect of each predictor on \\(Y\\). This can cause large variances in \\(\\hat{\\beta}\\), leading to unreliable estimates.\n\n\n\nFor large samples, \\(\\hat{\\beta}\\) is approximately normally distributed:\n\\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\sim N(0, V)\n\\]\nwhere \\(V = Q^{-1} \\Sigma Q^{-1}\\), with \\(Q = E[X'X]\\) and \\(\\Sigma = E[u^2 X X']\\)."
  },
  {
    "objectID": "linear-regression.html#linear-regression",
    "href": "linear-regression.html#linear-regression",
    "title": "linear regression",
    "section": "",
    "text": "Linear regression is a fundamental statistical method for estimating the relationship between a dependent variable and one or more independent variables.\n\n\nThe population regression function describes the expected value of the outcome \\(Y\\) given the value of \\(X\\):\n\\[\nE[Y | X] = \\beta_0 + \\beta_1 X\n\\]\nFor a binary variable \\(X\\), this function indicates the difference in \\(Y\\) between two groups: - \\(E[Y | X = 1] - E[Y | X = 0] = \\beta_1\\)\nFor multiple variables (e.g., education \\(X_1\\) and experience \\(X_2\\)), the function becomes:\n\\[\nE[Y | X] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\n\n\nThe Law of Iterated Expectations states:\n\\[\nE[Y] = E[E[Y | X]]\n\\]\nFor binary \\(X\\), it simplifies to:\n\\[\nE[Y] = E[Y | X = 1] \\cdot P(X = 1) + E[Y | X = 0] \\cdot P(X = 0)\n\\] Where \\(P\\) is a proportion.\n\n\n\nThe regression model includes an error term \\(u\\) to capture the variation in \\(Y\\) unexplained by \\(X\\):\n\\[\nY = \\beta_0 + \\beta_1 X + u\n\\]\nwhere \\(E[u | X] = 0\\). The full model for multiple variables \\(X_1, X_2, \\ldots, X_k\\) is:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + u\n\\]\n\n\n\nThe OLS estimator minimizes the sum of squared residuals (differences between observed and predicted values):\n\\[\n\\hat{\\beta} = \\text{argmin}_\\beta \\sum_{i=1}^n (Y_i - X_i' \\beta)^2\n\\]\nwhere \\(X_i'\\) is the vector of regressors for observation \\(i\\). The solution for \\(\\hat{\\beta}\\), the OLS estimator, is:\n\\[\n\\hat{\\beta} = (X'X)^{-1} X'Y\n\\]\n\n\nFor a simple linear model with one regressor:\n\\[\nY = \\beta_0 + \\beta_1 X + u\n\\]\nThe slope coefficient \\(\\beta_1\\) is:\n\\[\n\\beta_1 = \\frac{\\text{cov}(Y, X)}{\\text{var}(X)}\n\\]\nand the intercept \\(\\beta_0\\) is:\n\\[\n\\beta_0 = E[Y] - \\beta_1 E[X]\n\\]\n\n\n\n\n\nFitted values: \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\)\nResiduals: \\(\\hat{u}_i = Y_i - \\hat{Y}_i\\)\n\n\n\n\n\nUnbiasedness: Under the assumption \\(E[u | X] = 0\\), the OLS estimator is unbiased: \\[\nE[\\hat{\\beta}] = \\beta\n\\]\nConsistency: As the sample size \\(n \\to \\infty\\), \\(\\hat{\\beta}\\) converges to the true \\(\\beta\\).\n\n\n\n\nIf \\(u\\) has constant variance (homoskedasticity), the variance of \\(\\hat{\\beta}\\) is:\n\\[\n\\text{var}(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}\n\\]\nwhere \\(\\sigma^2 = \\text{var}(u)\\). Under heteroskedasticity, robust standard errors are often used instead.\n\n\n\nTo test if a coefficient \\(\\beta_j = 0\\), we calculate the t-statistic:\n\\[\nt = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)}\n\\]\nwhere \\(\\text{se}(\\hat{\\beta}_j)\\) is the standard error of \\(\\hat{\\beta}_j\\). For a 5% significance level, if \\(|t| &gt; 1.96\\), we reject the null hypothesis \\(\\beta_j = 0\\).\n\n\n\n\nR-Squared: Measures the proportion of variance in \\(Y\\) explained by \\(X\\). \\[\nR^2 = 1 - \\frac{\\sum_{i=1}^n \\hat{u}_i^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}\n\\]\nInterpretation: An \\(R^2\\) close to 1 suggests a strong relationship between \\(Y\\) and \\(X\\), while an \\(R^2\\) close to 0 suggests a weak relationship.\n\n\n\n\nMulticollinearity occurs when predictors in \\(X\\) are highly correlated, making it difficult to isolate the effect of each predictor on \\(Y\\). This can cause large variances in \\(\\hat{\\beta}\\), leading to unreliable estimates.\n\n\n\nFor large samples, \\(\\hat{\\beta}\\) is approximately normally distributed:\n\\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\sim N(0, V)\n\\]\nwhere \\(V = Q^{-1} \\Sigma Q^{-1}\\), with \\(Q = E[X'X]\\) and \\(\\Sigma = E[u^2 X X']\\)."
  },
  {
    "objectID": "linear-regression.html#deriving-the-argmin-function-in-linear-regression",
    "href": "linear-regression.html#deriving-the-argmin-function-in-linear-regression",
    "title": "linear regression",
    "section": "Deriving the argmin Function in Linear Regression",
    "text": "Deriving the argmin Function in Linear Regression\nIn the context of Ordinary Least Squares (OLS) regression, the argmin function identifies the values of the coefficients \\(\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_k)\\) that minimize the average squared difference between the observed values and the values predicted by the model.\n\nStep 1: Setting up the Problem\nTo find the best linear approximation of \\(Y\\) in terms of \\(X\\), we express \\(Y\\) as:\n\\[\nY = X'\\beta + u\n\\]\nwhere: - \\(Y\\) is the dependent variable. - \\(X\\) is a matrix of independent variables (each row represents an observation, and each column represents a variable). - \\(\\beta\\) is the vector of coefficients we aim to estimate. - \\(u\\) is the error term, assumed to have a mean of zero, i.e., \\(E[u | X] = 0\\).\nGiven this setup, we seek the \\(\\beta\\) that minimizes the expected squared error, or the mean squared error function:\n\\[\n\\beta = \\text{argmin}_{b \\in \\mathbb{R}^k} E[(Y - X'b)^2]\n\\]\n\n\nStep 2: Using the Property of Expectations to Define the Objective\nFor any value \\(c\\), we know that:\n\\[\nE[(Y - E[Y])^2] \\leq E[(Y - c)^2]\n\\]\nThis property implies that the optimal \\(b\\) minimizes the squared deviation of \\(Y\\) from its conditional expectation \\(E[Y | X]\\). Hence, \\(\\beta\\) is the value that minimizes:\n\\[\nE[(Y - X'b)^2]\n\\]\nThis function is minimized when \\(E[Y | X] = X'\\beta\\), which forms the basis of the OLS estimation.\n\n\nStep 3: Sample-Based Minimization (Estimation with Observed Data)\nIn practice, we estimate \\(\\beta\\) using a sample of data points \\((Y_1, X_1), (Y_2, X_2), \\ldots, (Y_n, X_n)\\). The sample-based version of the minimization problem becomes:\n\\[\n\\hat{\\beta} = \\text{argmin}_{b \\in \\mathbb{R}^k} \\frac{1}{n} \\sum_{i=1}^n (Y_i - X_i'b)^2\n\\]\nAlternatively, this can be simplified as:\n\\[\n\\hat{\\beta} = \\text{argmin}_{b \\in \\mathbb{R}^k} \\sum_{i=1}^n (Y_i - X_i'b)^2\n\\]\n\n\nStep 4: Solving for \\(\\hat{\\beta}\\)\nTo find the values of \\(\\hat{\\beta}\\) that minimize the sum of squared residuals, we differentiate the objective function with respect to each element of \\(\\beta\\), set these derivatives to zero, and solve for \\(\\beta\\). This yields the OLS estimator:\n\\[\n\\hat{\\beta} = (X'X)^{-1} X'Y\n\\]\nThis solution for \\(\\hat{\\beta}\\) provides the regression coefficients that minimize the sum of squared residuals, making it the best linear predictor of \\(Y\\) based on \\(X\\)."
  }
]